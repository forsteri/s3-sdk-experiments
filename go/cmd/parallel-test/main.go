package main

import (
	"context"
	"flag"
	"fmt"
	"log"
	"math"
	"os"
	"time"

	"s3-uploader/internal/aws"
	"s3-uploader/internal/logger"
	"s3-uploader/internal/models"
	"s3-uploader/internal/uploader"
)

func main() {
	// ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
	var (
		configFile   = flag.String("config", "config.json", "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
		source       = flag.String("source", "", "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
		bucket       = flag.String("bucket", "s3-experiment-bucket-250615", "S3ãƒã‚±ãƒƒãƒˆå")
		key          = flag.String("key", "", "S3ã‚­ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã¾ãŸã¯ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰")
		recursive    = flag.Bool("recursive", false, "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
		dryRun       = flag.Bool("dry-run", false, "ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰")
		parallel     = flag.Bool("parallel", true, "ä¸¦åˆ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä½¿ç”¨")
		workers      = flag.Int("workers", 0, "ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆ0ã®å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å€¤ã‚’ä½¿ç”¨ï¼‰")
		benchmarkDir = flag.String("benchmark", "", "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæŒ‡å®šæ™‚ã¯ä¸¦åˆ—/é †æ¬¡ã®æ¯”è¼ƒã‚’å®Ÿè¡Œï¼‰")
	)
	flag.Parse()

	if *source == "" && *benchmarkDir == "" {
		log.Fatal("âŒ -source ã¾ãŸã¯ -benchmark ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
	}

	// è¨­å®šã‚’èª­ã¿è¾¼ã¿
	cfg, err := models.LoadFromFile(*configFile)
	if err != nil {
		log.Fatalf("è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: %v", err)
	}

	// ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¨­å®šã‚’ä¸Šæ›¸ã
	if *dryRun {
		cfg.Options.DryRun = true
	}
	if *workers > 0 {
		cfg.Options.ParallelUploads = *workers
	}
	if !*parallel {
		cfg.Options.ParallelUploads = 1
	}

	// ãƒ­ã‚¬ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
	_, err = logger.Setup(cfg.Logging)
	if err != nil {
		log.Fatalf("âŒ ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: %v", err)
	}

	lgr := logger.GetLogger()

	// S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½œæˆ
	clientManager, err := aws.NewClientManager(cfg.AWS)
	if err != nil {
		lgr.Fatalf("S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: %v", err)
	}

	ctx := context.Background()

	// ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰
	if *benchmarkDir != "" {
		runBenchmark(ctx, clientManager, cfg, *benchmarkDir, *bucket)
		return
	}

	// é€šå¸¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰
	uploaderInstance := uploader.NewUploader(clientManager, cfg.Options)

	// ã‚½ãƒ¼ã‚¹ã®å­˜åœ¨ç¢ºèª
	sourceInfo, err := os.Stat(*source)
	if err != nil {
		lgr.Fatalf("âŒ ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: %v", err)
	}

	fmt.Println("ğŸš€ Parallel Upload Test")
	fmt.Println("========================================")
	fmt.Printf("ğŸ“ Source: %s\n", *source)
	fmt.Printf("ğŸª£ Bucket: %s\n", *bucket)
	fmt.Printf("ğŸ”‘ Key/Prefix: %s\n", *key)
	fmt.Printf("ğŸ‘· Workers: %d\n", cfg.Options.ParallelUploads)
	fmt.Printf("ğŸƒ Dry Run: %v\n", cfg.Options.DryRun)
	fmt.Println("========================================")

	startTime := time.Now()

	if sourceInfo.IsDir() {
		// ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
		if *key == "" {
			*key = "parallel-test/"
		}

		results, err := uploaderInstance.UploadDirectoryWithRetry(ctx, *source, *bucket, *key, *recursive)
		if err != nil {
			lgr.Fatalf("âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: %v", err)
		}

		// çµæœã®é›†è¨ˆ
		printResults(results, startTime)
	} else {
		// å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
		if *key == "" {
			*key = fmt.Sprintf("parallel-test/%s", sourceInfo.Name())
		}

		result, err := uploaderInstance.UploadFileWithRetry(ctx, *source, *bucket, *key)
		if err != nil {
			lgr.Fatalf("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: %v", err)
		}

		printResults([]uploader.UploadResult{*result}, startTime)
	}
}

// runBenchmark ä¸¦åˆ—å‡¦ç†ã¨é †æ¬¡å‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
func runBenchmark(ctx context.Context, clientManager aws.S3Operations, cfg *models.Config, source string, bucket string) {
	lgr := logger.GetLogger()

	fmt.Println("ğŸ Benchmark Mode")
	fmt.Println("========================================")
	fmt.Printf("ğŸ“ Source: %s\n", source)
	fmt.Printf("ğŸª£ Bucket: %s\n", bucket)
	fmt.Println("========================================")

	// é †æ¬¡å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
	cfg.Options.ParallelUploads = 1
	sequentialUploader := uploader.NewUploader(clientManager, cfg.Options)

	fmt.Println("ğŸ“Š Sequential Upload (1 worker)")
	fmt.Println("----------------------------------------")
	startTime := time.Now()
	results, err := sequentialUploader.UploadDirectoryWithRetry(ctx, source, bucket, "benchmark-sequential/", true)
	if err != nil {
		lgr.Error("Sequential upload failed", "error", err)
	} else {
		sequentialDuration := time.Since(startTime)
		printBenchmarkResult("Sequential", results, sequentialDuration)
	}

	// ä¸¦åˆ—å‡¦ç†ã®ãƒ†ã‚¹ãƒˆï¼ˆç•°ãªã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼‰
	workerCounts := []int{2, 4, 8, 16}
	for _, workers := range workerCounts {
		cfg.Options.ParallelUploads = workers
		parallelUploader := uploader.NewUploader(clientManager, cfg.Options)

		fmt.Printf("\nğŸ“Š Parallel Upload (%d workers)\n", workers)
		fmt.Println("----------------------------------------")
		startTime := time.Now()
		results, err := parallelUploader.UploadDirectoryWithRetry(ctx, source, bucket, fmt.Sprintf("benchmark-parallel-%d/", workers), true)
		if err != nil {
			lgr.Error("Parallel upload failed", "workers", workers, "error", err)
		} else {
			parallelDuration := time.Since(startTime)
			printBenchmarkResult(fmt.Sprintf("Parallel-%d", workers), results, parallelDuration)
		}
	}
}

// printResults ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµæœã‚’è¡¨ç¤º
func printResults(results []uploader.UploadResult, startTime time.Time) {
	duration := time.Since(startTime)
	totalFiles := len(results)
	successFiles := 0
	failedFiles := 0
	skippedFiles := 0
	totalBytes := int64(0)

	for _, result := range results {
		if result.Success {
			if result.SkippedReason != "" {
				skippedFiles++
			} else {
				successFiles++
				totalBytes += result.Size
			}
		} else {
			failedFiles++
		}
	}

	fmt.Println("\nğŸ“Š Upload Results")
	fmt.Println("========================================")
	fmt.Printf("â±ï¸  Duration: %s\n", duration.Round(time.Millisecond))
	fmt.Printf("ğŸ“ Total Files: %d\n", totalFiles)
	fmt.Printf("âœ… Successful: %d\n", successFiles)
	fmt.Printf("âŒ Failed: %d\n", failedFiles)
	fmt.Printf("â­ï¸  Skipped: %d\n", skippedFiles)
	fmt.Printf("ğŸ“¦ Total Size: %s\n", formatBytes(totalBytes))
	// æœ€å°å€¤ã‚’1ãƒŸãƒªç§’ã¨ã—ã¦è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—ã‚’é˜²ãï¼‰
	seconds := math.Max(duration.Seconds(), 0.001)
	throughput := float64(totalBytes) / seconds
	fmt.Printf("ğŸš€ Throughput: %s/s\n", formatBytes(int64(throughput)))

	// å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°
	if failedFiles > 0 {
		fmt.Println("\nâŒ Failed Files:")
		count := 0
		for _, result := range results {
			if !result.Success && result.Error != nil {
				fmt.Printf("  - %s: %v\n", result.Source, result.Error)
				count++
				if count >= 5 && failedFiles > 5 {
					fmt.Printf("  ... and %d more\n", failedFiles-5)
					break
				}
			}
		}
	}
}

// printBenchmarkResult ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’è¡¨ç¤º
func printBenchmarkResult(mode string, results []uploader.UploadResult, duration time.Duration) {
	totalFiles := 0
	successFiles := 0
	totalBytes := int64(0)

	for _, result := range results {
		totalFiles++
		if result.Success && result.SkippedReason == "" {
			successFiles++
			totalBytes += result.Size
		}
	}

	fmt.Printf("âœ… %s completed in %s\n", mode, duration.Round(time.Millisecond))

	// æœ€å°å€¤ã‚’1ãƒŸãƒªç§’ã¨ã—ã¦è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—ã‚’é˜²ãï¼‰
	seconds := math.Max(duration.Seconds(), 0.001)

	throughput := float64(totalBytes) / seconds
	filesPerSecond := float64(successFiles) / seconds

	fmt.Printf("   Files: %d/%d (%.1f files/s)\n", successFiles, totalFiles, filesPerSecond)
	fmt.Printf("   Data: %s (%.1f MB/s)\n", formatBytes(totalBytes), throughput/1024/1024)
}

// formatBytes ãƒã‚¤ãƒˆæ•°ã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
func formatBytes(bytes int64) string {
	const (
		KB = 1024
		MB = KB * 1024
		GB = MB * 1024
	)

	switch {
	case bytes >= GB:
		return fmt.Sprintf("%.2f GB", float64(bytes)/GB)
	case bytes >= MB:
		return fmt.Sprintf("%.2f MB", float64(bytes)/MB)
	case bytes >= KB:
		return fmt.Sprintf("%.2f KB", float64(bytes)/KB)
	default:
		return fmt.Sprintf("%d B", bytes)
	}
}
